# Final-Year-Project-Visual-Storytelling-

This project implements and evaluates four different image captioning models: [model 1], [model 2], [model 3], and BLIP. The models are evaluated based on the following metrics: [list metrics].

The main aim of the project is to create a web app that is targeted towards blind people. The web app has a caption to speech output and visual question answering (VQA) capability in addition to the model comparison.
Setup

To run the project, you will need to install the following dependencies:

    [dependency 1]
    [dependency 2]
    [dependency 3]
    [dependency 4]

You will also need to download the following data:

    [data 1]
    [data 2]
    [data 3]

Once you have installed the dependencies and downloaded the data, you can run the project by running the following command:

css

python main.py

## Models
Model 1

[Description of model 1]
## Model 2

[Description of model 2]
## Model 3

[Description of model 3]
## CNN-LSTM

[Description of BLIP model]
## Evaluation Metrics

The models are evaluated based on the following metrics:

    [Metric 1]: [Explanation of metric 1]
    [Metric 2]: [Explanation of metric 2]
    [Metric 3]: [Explanation of metric 3]

## Results

The results of the evaluation are as follows:
Model	Metric 1	Metric 2	Metric 3
Model 1	[Value]	[Value]	[Value]
Model 2	[Value]	[Value]	[Value]
Model 3	[Value]	[Value]	[Value]
BLIP	[Value]	[Value]	[Value]

Based on the evaluation metrics, the BLIP model performed the best.
Web App

The web app has the following capabilities:
Caption to Speech Output

The caption to speech output capability allows blind users to hear the image captions read out loud. The user can input an image and the web app will generate a caption using the selected model, and then read out the caption using a text-to-speech engine.
Visual Question Answering (VQA)

The VQA capability allows blind users to ask questions about an image and get an answer. The user can input an image and a question, and the web app will generate an answer using the selected model.
Usage

To use the web app, run the following command:

python app.py

This will launch a local web server that you can access in your web browser.
## Conclusion

In this project, we implemented and evaluated four different image captioning models. Based on the evaluation metrics, the BLIP model performed the best. Additionally, we created a web app targeted towards blind people that has a caption to speech output and VQA capability in addition to the model comparison.
